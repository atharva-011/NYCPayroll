# Data

```{r message=FALSE, warning=FALSE}
library(httr)
library(jsonlite)
library(knitr)
library(redav)
library(mi)
library(ggplot2)
library(data.table)
```

## Technical Description

The data that we will be using to analyze Payroll Data is from the NYC Open Data [website](https://opendata.cityofnewyork.us/) and is titled as  [**Citywide Payroll Data (Fiscal Year)**](https://data.cityofnewyork.us/City-Government/Citywide-Payroll-Data-Fiscal-Year-/k397-673e). The data set was created back in 2015 and has been constantly been updated on a annual basis (last update was on November 28,2023). It tells us about the amount of money spent on salaries and overtime pay for all municipal employees based in New York City. The data is provided by **Office of Payroll Administration (OPA)**. 

The data can be downloaded as a [JSON file](https://data.cityofnewyork.us/resource/k397-673e.json) from the website mentioned above. We make an API request with this URL, to obtain the data and convert it to a Dataframe for further processing.




```{r}
#importing the dataset
nyc_payroll_data <- fread("C:/Users/athar/Desktop/Columbia Sem1/edav/nyc_payroll_data.csv")

# Random sample of 10000 rows
randomized_data <- nyc_payroll_data[sample(nrow(nyc_payroll_data)), ]
nyc_payroll_data <- randomized_data[1:10000, ]

# Print the first few rows of the random downsized data
print(head(nyc_payroll_data))

```

```{r}
print(dim(nyc_payroll_data))

```

The data consists of 5.66 million rows (essentially 5.66 million municipal employees) and 17 columns. Every row basically tells us about the employee salary, their work location, agency, base pay, overtime (if any), etc. However, the data size is too large for efficient processing hence we random sample our data to the size of 10000 samples for our study.

```{r}
# Display columns of the dataset
kable(data.frame(Column_Names = names(nyc_payroll_data)), "markdown")

```

Presented above are the columns of the dataset.

On further examination of the data, we find that there are few inconsistencies in the data:

- The salaries for certain employees have not been mentioned per annum but on a per day or hour basis. Hence, we need to scale the data accordingly, to ensure that the data is uniform and fair comparisons can be made.

- There exist some NAN values in the data.

## Research Plan

* To understand the employee compensation, we can use a histogram to study the distribution of salaries.

* While understanding the distribution of total pay is important, we also plan to see how these salaries are broken down and how various components of these salaries are distributed, using Scatter Plots. 

* We can also use grouped bar charts, to understand these relationships over time.

* Using a heatmap, with the budget as the color scale, we will be able to uncover any biases when it comes to salaries based on boroughs and agencies.

* A Mosaic plot will be ideal to unravel patterns and compare multivariate relationships when it comes to the salaries earned based on the Job Title and the boroughs.

* A faceted scatterplot, or a faceted barplot would help us present insights on how the relationship between overtime pay and the corresponding payroll has evolved over the years.

* Since we can show the boroughs of NYC using an actual map, we plan to categorize an individual's work experience, and then use that to show how their compensation varies in different parts of NYC, w.r.t. their work experience.

* We also aim to analyze the leave status and workforce dynamics. Leave status typically indicates whether an employee is on leave or not, and if so, what type of leave they are on. This information is often valuable for HR and management to understand workforce availability, plan for staffing needs, and monitor employee well-being.

* We also plan to make use of D3 to make some other interactive graphs showing some other relationships.


## Missing Value Analysis

Describe any patterns you discover in missing values. If no values are missing, graphs should still be included showing that.

(suggested: 2 graphs plus commentary)
Showing count of NAN values per column

```{r}
colSums(is.na(nyc_payroll_data)) |> sort(decreasing = TRUE)
```

We see that only one column, namely `mid_init` has missing values. This feature represents the middle initial of the person under consideration. 

```{r, fig.height=18, fig.width=30}
plot_missing(nyc_payroll_data, percent = FALSE)
```

As can be seen from the above plot, there are only two patterns which can be observed in our data. The first being the complete lack of NaN values, whereas the second pattern exhibits missing values in one column only, namely, `mid_init`.

The graph on the right shows that there are nearly 300 rows exhibiting the second pattern, and about 700 points exhibiting the first, where they do not have any missing values at all.

We also get to see which columns have missing values in the form of a bar chart on top, and see that only one column falls under this category.

```{r, fig.height=11}
missing_data.frame(nyc_payroll_data) |> image()
```


This graph shows us the missing values in the entire dataset. The cells represent the actual values after scaling, defined by each row and column of the actual dataset. Higher values take on the lighter colors, whereas colors close to red represent values closer to -1. 

The values in black, indicate missing data. Here, we see that `mid_init` has missing data for nearly 300 hundred contiguous rows.

Although the absolute reason for the same is unknown, it could be possible that this data was not collected at all in that period. This can be inferred from the pattern.
